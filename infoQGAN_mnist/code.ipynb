{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import cv2\n",
    "from functools import reduce\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====전처리 이전=====\n",
      "number of training data :  (60000, 28, 28) (60000,)\n",
      "number of test data :  (10000, 28, 28) (10000,)\n",
      "train shape =  (1000, 16, 16)\n",
      "test shape =  (1135, 16, 16)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "# 1. 데이터 불러오기\n",
    "train_data = datasets.MNIST(root='./data/02/', train=True, download=True)\n",
    "test_data = datasets.MNIST(root='./data/02/', train=False, download=True)\n",
    "x_train = train_data.data.numpy()\n",
    "y_train = train_data.targets.numpy()\n",
    "x_test = test_data.data.numpy()\n",
    "y_test = test_data.targets.numpy()\n",
    "print('====전처리 이전=====')\n",
    "print('number of training data : ', x_train.shape, y_train.shape)\n",
    "print('number of test data : ', x_test.shape, y_test.shape)\n",
    "\n",
    "#######\n",
    "train_size = 1000\n",
    "imgw = 16\n",
    "#######\n",
    "\n",
    "# 0만 고른다\n",
    "GENERATING_NUMBER = 1\n",
    "x_train_origin = x_train[y_train == GENERATING_NUMBER]\n",
    "x_train_origin = x_train_origin[:train_size]\n",
    "x_test_origin = x_test[y_test == GENERATING_NUMBER]\n",
    "x_train_origin, x_test_origin = x_train_origin/255.0, x_test_origin/255.0\n",
    "\n",
    "\n",
    "# 4x4로 resize\n",
    "x_train = np.array([cv2.resize(img, dsize=(imgw, imgw)) for img in x_train_origin])\n",
    "x_test = np.array([cv2.resize(img, dsize=(imgw, imgw)) for img in x_test_origin])\n",
    "\n",
    "# 최대값 1로 고정\n",
    "# add noise\n",
    "x_train = x_train + np.random.normal(0, 0.1, size=x_train.shape)\n",
    "x_train = x_train / np.max(x_train, axis=(1, 2), keepdims=True)\n",
    "x_test = x_test / np.max(x_test, axis=(1, 2), keepdims=True)\n",
    "\n",
    "print(\"train shape = \", x_train.shape)\n",
    "print(\"test shape = \", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing the required library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Creating a figure with two subplots\n",
    "fig, axs = plt.subplots(1,2, figsize=(4, 8))\n",
    "t = np.random.randint(0, train_size)\n",
    "axs[0].imshow(x_train_origin[t])\n",
    "axs[1].imshow(x_train[t])\n",
    "plt.title(f'origin-reshape img of {t}th')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_qubits = 9 n_layers = 20\n"
     ]
    }
   ],
   "source": [
    "noise_qubits = 7\n",
    "code_qubits = 2\n",
    "output_qubits = 8\n",
    "n_qubits = noise_qubits + code_qubits\n",
    "assert(output_qubits <= n_qubits) # 출력 큐빗은 noise qubit이하여야 한다.\n",
    "\n",
    "n_layers = 20\n",
    "BATCH_SIZE = 16\n",
    "print(\"n_qubits = {} n_layers = {}\".format(n_qubits, n_layers))\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "random_indices = np.random.choice(2**n_qubits, 2 * (2**output_qubits), replace=False)\n",
    "mask = np.zeros(2**n_qubits, dtype=bool)\n",
    "mask_inv = np.zeros(2**n_qubits, dtype=bool)\n",
    "mask[random_indices[:len(random_indices)//2]] = True # amplitude embedding에 사용할 위치는 랜덤하게 고른다.\n",
    "mask_inv[random_indices[len(random_indices)//2:]] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 0.2\n",
    "\n",
    "def generator_init(generator_input):\n",
    "    for i in range(n_qubits):\n",
    "        qml.RY(generator_input[i]*np.pi/2, wires=i) # TODO: *a 해서 값 범위 맞추기\n",
    "\n",
    "def generator_layer(params):\n",
    "    for i in range(n_qubits):\n",
    "        qml.RX(params[i][0], wires=i)\n",
    "        qml.RY(params[i][1], wires=i)\n",
    "        qml.RZ(params[i][2], wires=i)\n",
    "    \n",
    "    for i in range(n_qubits):\n",
    "        qml.CNOT(wires=[i, (i+1)%n_qubits])\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def generator_circuit(params, generator_input):\n",
    "    \"\"\"\n",
    "    quantum circuit nodeq1\n",
    "    generator_input (np.array(큐빗)) : 생성기 입력 seed (noise + code)\n",
    "    params (torch.Tensor(레이어,큐빗,3)): a parameter\n",
    "    마지막 측정은 모두 Z로\n",
    "    \"\"\"\n",
    "\n",
    "    generator_init(generator_input)\n",
    "\n",
    "    for param in params:\n",
    "        generator_layer(param)\n",
    "\n",
    "    return qml.probs(wires=range(n_qubits))\n",
    "\n",
    "def generator_forward(params, generator_input):\n",
    "    # 제너레이터 돌리고 결과 return하는 함수\n",
    "    generator_output = [generator_circuit(params, single_in) for single_in in generator_input] # (BATCH_SIZE, 2**n_qubits) 차원\n",
    "    generator_output = torch.stack(generator_output) # (BATCH_SIZE, 2**n_qubits) 차원\n",
    "\n",
    "    output = generator_output[:, mask]\n",
    "    output /= generator_output.sum(dim=1, keepdim=True) # (BATCH_SIZE, n_qubits) 차원\n",
    "    # 곱하기 2 해서 범위를 초과하게 만들음\n",
    "    #output = generator_output[:, mask] / generator_output[:, mask_inv]\n",
    "    #output[output <= output.mean()] = 0\n",
    "    \n",
    "    output = output / output.max(dim=1, keepdim=True)[0]\n",
    "\n",
    "    return output, generator_output[:, -code_qubits:] # noise, code 순서로 반환\n",
    "\n",
    "\n",
    "def generator_train_step(params, generator_input, use_mine = False, _qmine = False):\n",
    "    '''\n",
    "    params (torch.Tensor(레이어,큐빗,3)): a parameter\n",
    "    generator_input (torch.Tensor(BATCH_SIZE, n_qubits)): 생성기 입력 seed (noise + code). -1~1 사이의 값\n",
    "    '''\n",
    "    code_input = generator_input[:, -code_qubits:] # 입력중에서 code만 뽑는다. (BATCH_SIZE, code_qubits)\n",
    "\n",
    "    generator_output, code_output = generator_forward(params, generator_input) # 출력을 뽑아낸다\n",
    "    generator_output = generator_output.to(torch.float32) # (BATCH_SIZE, output_qubits)\n",
    "    \n",
    "    disc_output = discriminator(generator_output) # 밑에 코드에서 정의됨\n",
    "    gan_loss = torch.log(1-disc_output).mean()\n",
    "    # print(\"gan_loss = \", gan_loss, gan_loss.shape)\n",
    "    \n",
    "    t = (code_input - code_output).pow(2).mean()\n",
    "\n",
    "    if use_mine:\n",
    "        pred_xy = mine(code_input, generator_output)\n",
    "        code_input_shuffle = code_input[torch.randperm(BATCH_SIZE)]\n",
    "        pred_x_y = mine(code_input_shuffle, generator_output)\n",
    "        mi = torch.mean(pred_xy) - torch.log(torch.mean(torch.exp(pred_x_y)))\n",
    "        gan_loss -= coeff * mi\n",
    "\n",
    "    elif _qmine:\n",
    "        gan_loss += 0 # TODO: qmine loss 추가하기\n",
    "\n",
    "    return generator_output, gan_loss+t*0, gan_loss, t # TODO: 이건 분석용으로 넣어놓음.지워야 함.\n",
    "\n",
    "\n",
    "def prediction(params, image, debug=False):\n",
    "    prob_0 = 0\n",
    "    prob_1 = 0\n",
    "    for i in range(n_layers):\n",
    "        prob_0 += circuit(params[:i+1], (image, 0))\n",
    "        prob_1 += circuit(params[:i+1], (image, 1))\n",
    "\n",
    "    if (debug):\n",
    "        return (int(prob_0 <= prob_1), prob_0, prob_1)\n",
    "    return int(prob_0 <= prob_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class LinearDiscriminator(nn.Module):\n",
    "    def __init__(self, input_dim=2**output_qubits):\n",
    "        super().__init__()\n",
    "        self.hidden_size = 50\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, self.hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(self.hidden_size, 1),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        if(len(x.shape) != 2):\n",
    "            x = x.view(x.shape[0], -1)\n",
    "\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class LinearMine(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearMine, self).__init__()\n",
    "        H = 20\n",
    "        self.fc1 = nn.Linear(code_qubits, H)\n",
    "        self.fc2 = nn.Linear(2**output_qubits, H)\n",
    "        self.fc3 = nn.Linear(H, 1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        h1 = F.relu(self.fc1(x)+self.fc2(y))\n",
    "        h2 = self.fc3(h1)\n",
    "        return h2\n",
    "\n",
    "disc_loss_fn = nn.BCELoss()\n",
    "def disc_cost_fn(real_input, fake_input, smoothing=False):\n",
    "    batch_num = real_input.shape[0]\n",
    "    disc_real = discriminator(real_input)\n",
    "    disc_fake = discriminator(fake_input)\n",
    "\n",
    "    real_label = torch.ones((batch_num, 1))\n",
    "    fake_label = torch.zeros((batch_num, 1))\n",
    "    \n",
    "    if smoothing:\n",
    "        real_label = real_label - 0.2*torch.rand(real_label.shape).to(device)\n",
    "    \n",
    "    loss = 0.5 * (disc_loss_fn(disc_real, real_label) + disc_loss_fn(disc_fake, fake_label))\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter shape:  torch.Size([20, 9, 3])\n"
     ]
    }
   ],
   "source": [
    "G_lr = 1e-3\n",
    "D_lr = 1e-4\n",
    "M_lr = 1e-3\n",
    "use_mine = False\n",
    "use_qmine = False\n",
    "\n",
    "generator_params = Variable(torch.tensor(np.random.normal(-np.pi, np.pi, (n_layers, n_qubits, 3))), requires_grad=True)\n",
    "print(\"parameter shape: \", generator_params.shape)\n",
    "\n",
    "discriminator = LinearDiscriminator()\n",
    "mine = LinearMine()\n",
    "\n",
    "G_opt = torch.optim.Adam([generator_params], lr=G_lr)\n",
    "D_opt = torch.optim.Adam(discriminator.parameters(), lr=D_lr)\n",
    "M_opt = torch.optim.Adam(mine.parameters(), lr=M_lr)\n",
    "\n",
    "#G_scheduler = torch.optim.lr_scheduler.StepLR(G_opt, step_size=30, gamma=0.7)\n",
    "#D_scheduler = torch.optim.lr_scheduler.StepLR(D_opt, step_size=30, gamma=0.85)\n",
    "#M_scheduler = torch.optim.lr_scheduler.StepLR(M_opt, step_size=30, gamma=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "title = f'try6_{use_mine}'\n",
    "folder_paths = [f'result/{title}', f'result/{title}/savepoints', f'result/{title}/graphs', f'result/{title}/samples']\n",
    "for path in folder_paths:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "with open(f'result/{title}/param.txt', 'w') as f:\n",
    "    f.write('G_lr = {}\\n'.format(G_lr))\n",
    "    f.write('D_lr = {}\\n'.format(D_lr))\n",
    "    f.write('M_lr = {}\\n'.format(M_lr))\n",
    "    #f.write('G_scheduler: step={}, gamma={}\\n'.format(G_scheduler.step_size, G_scheduler.gamma))\n",
    "    #f.write('D_scheduler: step={}, gamma={}\\n'.format(D_scheduler.step_size, D_scheduler.gamma))\n",
    "    #f.write('M_scheduler: step={}, gamma={}\\n'.format(M_scheduler.step_size, M_scheduler.gamma))\n",
    "    f.write('coeff = {}\\n'.format(coeff))\n",
    "    f.write('use_mine = {}\\n'.format(use_mine))\n",
    "    f.write('use_qmine = {}\\n'.format(use_qmine))\n",
    "    f.write('n_qubits = {}\\n'.format(n_qubits))\n",
    "    f.write('output_qubits = {}\\n'.format(output_qubits))\n",
    "    f.write('code_qubits = {}\\n'.format(code_qubits))\n",
    "    f.write('n_layers = {}\\n'.format(n_layers))\n",
    "    f.write('param shape = {}\\n'.format(generator_params.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "def visualize_output(log_gen_outputs, log_gen_codes, title, rep, recorder):\n",
    "\n",
    "    # 이미지 리스트에서 랜덤하게 8개 이미지 선택\n",
    "    num_images = 8\n",
    "    image_list = log_gen_outputs\n",
    "    image_indices = np.random.choice(len(image_list), num_images, replace=False)\n",
    "    selected_images = [image_list[i] for i in image_indices]\n",
    "\n",
    "    # 선택된 이미지를 8x8 크기로 reshape\n",
    "    reshaped_images = [image.reshape(imgw, imgw) for image in selected_images]\n",
    "\n",
    "    # 이미지를 2x4 배치로 그리기 위한 그리드 설정\n",
    "    grid_rows = 2\n",
    "    grid_cols = 4\n",
    "    gs = GridSpec(grid_rows, grid_cols)\n",
    "\n",
    "    # 이미지를 그리드에 배치하고 시각화\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, image in enumerate(reshaped_images):\n",
    "        row = i // grid_cols\n",
    "        col = i % grid_cols\n",
    "        ax = plt.subplot(gs[row, col])\n",
    "        ax.imshow(image)  # 이미지를 회색조로 표시\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"Epoch {rep}, random imgs\", fontsize=16)\n",
    "    plt.savefig(f'result/{title}/samples/{rep}.png')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(recorder['g_loss'], label='g_loss', marker='o')\n",
    "    plt.plot(recorder['d_loss'], label='d_loss', marker='o')\n",
    "    plt.plot(recorder['mi'], label='mi', marker='o')\n",
    "    plt.legend()\n",
    "    plt.title(f'Epoch {rep}, loss graph')\n",
    "    plt.savefig(f'result/{title}/graphs/{rep}.png')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/31 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "epoch = 300\n",
    "\n",
    "recorder_keywords = ['d_loss', 'g_loss', 'a_loss', 'b_loss', 'mi']\n",
    "recorder = {k: [] for k in recorder_keywords}\n",
    "final_rep = 0\n",
    "\n",
    "for rep in range(1, epoch+1):\n",
    "    np.random.shuffle(x_train)\n",
    "    iter_num = int(len(x_train) * 0.5 //BATCH_SIZE) # 매번 50% 추출해서 학습. 셔플하니까 자투리 생기는건 무시.\n",
    "    \n",
    "    G_loss_sum = 0.0\n",
    "    D_loss_sum = 0.0\n",
    "    a_loss_sum = 0.0\n",
    "    b_loss_sum = 0.0\n",
    "    mi_sum = 0.0\n",
    "    log_gen_outputs = []\n",
    "    log_gen_codes = []\n",
    "    pbar = tqdm(range(iter_num))\n",
    "    \n",
    "    for i in pbar:\n",
    "        batch = torch.FloatTensor(x_train[BATCH_SIZE * i : BATCH_SIZE * i + BATCH_SIZE])\n",
    "        \n",
    "        # train generator\n",
    "        generator_seed = torch.rand((BATCH_SIZE, n_qubits)) * 2 - 1\n",
    "        generator_output, generator_loss, a, b = generator_train_step(generator_params, generator_seed, use_mine=use_mine, _qmine=use_qmine)\n",
    "        G_opt.zero_grad()\n",
    "        generator_loss.requires_grad_(True)\n",
    "        generator_loss.backward()\n",
    "        G_opt.step()\n",
    "        a_loss_sum += a.detach().numpy()\n",
    "        b_loss_sum += b.detach().numpy()\n",
    "\n",
    "        # train discriminator\n",
    "        fake_input = generator_output.detach().to(torch.float32)\n",
    "        disc_loss = disc_cost_fn(batch.reshape(-1, imgw**2), fake_input, smoothing=False)\n",
    "        D_opt.zero_grad()\n",
    "        disc_loss.requires_grad_(True)\n",
    "        disc_loss.backward()\n",
    "        D_opt.step()\n",
    "\n",
    "        # train mine\n",
    "        code_input = generator_seed[:, -code_qubits:] \n",
    "        pred_xy = mine(code_input, fake_input)\n",
    "        code_input_shuffle = code_input[torch.randperm(BATCH_SIZE)]\n",
    "        pred_x_y = mine(code_input_shuffle, fake_input)\n",
    "        mi = -torch.mean(pred_xy) + torch.log(torch.mean(torch.exp(pred_x_y)))\n",
    "        M_opt.zero_grad()\n",
    "        mi.requires_grad_(True)\n",
    "        mi.backward()\n",
    "        M_opt.step()\n",
    "\n",
    "\n",
    "        D_loss_sum += disc_loss.item()\n",
    "        G_loss_sum += generator_loss.item()\n",
    "        mi_sum -= mi.item() # (-1)곱해져 있어서 빼야함.\n",
    "        a_loss_sum += a.item()\n",
    "        b_loss_sum += b.item()\n",
    "\n",
    "        pbar.set_postfix({'G_loss': G_loss_sum/(i+1), 'D_loss': D_loss_sum/(i+1), 'MI': mi_sum/(i+1)})\n",
    "        log_gen_outputs.append(fake_input.numpy())\n",
    "        log_gen_codes.append(code_input.numpy())\n",
    "\n",
    "    G_scheduler.step()\n",
    "    D_scheduler.step()\n",
    "    M_scheduler.step()\n",
    "\n",
    "    recorder['d_loss'].append(D_loss_sum/iter_num)\n",
    "    recorder['g_loss'].append(G_loss_sum/iter_num)\n",
    "    recorder['a_loss'].append(a_loss_sum/iter_num)\n",
    "    recorder['b_loss'].append(b_loss_sum/iter_num)\n",
    "    recorder['mi'].append(mi_sum/iter_num)\n",
    "    \n",
    "    log_gen_outputs = np.concatenate(log_gen_outputs, axis=0)\n",
    "    log_gen_codes = np.concatenate(log_gen_codes, axis=0)\n",
    "    print(\"epoch: {}, D_loss: {}, G_loss: {}, MI = {}\".format(rep, D_loss_sum/iter_num, G_loss_sum/iter_num, mi_sum/iter_num))\n",
    "    print(\"a_loss: {}, b_loss: {}\".format(a_loss_sum, b_loss_sum))\n",
    "\n",
    "    visualize_output(log_gen_outputs, log_gen_codes, title, rep, recorder)\n",
    "\n",
    "    with open(f'result/{title}/savepoints/discriminator_{rep}.pkl', 'wb') as file:\n",
    "        pickle.dump(discriminator, file)\n",
    "    with open(f'result/{title}/savepoints/generator_{rep}.pkl', 'wb') as file:\n",
    "        pickle.dump(generator_params, file)\n",
    "\n",
    "    final_rep = rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "df = pd.DataFrame(recorder)\n",
    "output_filename = f'result/{title}/recorder.xlsx'\n",
    "df.to_excel(output_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pennylane",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
