{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from functools import reduce\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 세팅\n",
    "name1 = \"AAPL\"\n",
    "name2 = \"TSLA\"\n",
    "cut_left = -0.1\n",
    "cut_right = 0.1\n",
    "num_bins = 16\n",
    "num_dataset = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 금융 데이터를 yfinance로부터 받아오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import datetime\n",
    "\n",
    "def get_stocks_datas(ticker_names: list[str], start_date, end_date):\n",
    "    # WARN: returns는 prices보다 길이가 1 작다.\n",
    "    prices = {}\n",
    "    returns = {}\n",
    "    for name in ticker_names:\n",
    "        data = yf.Ticker(name)\n",
    "        data = data.history(start=start_date, end=end_date, interval=\"1d\")\n",
    "        prices[name] = data['Close']\n",
    "        returns[name] = data['Close'].pct_change()[1:]\n",
    "    return prices, returns\n",
    "\n",
    "start_date = datetime.datetime(2011, 1, 1)\n",
    "end_date = datetime.datetime(2022, 12, 31)\n",
    "stock_prices, stock_returns = get_stocks_datas([name1, name2], start_date, end_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 분포의 특성 설정\n",
    "mean1 = 0.03\n",
    "mean2 = -0.03\n",
    "std_dev = 0.025\n",
    "correlation = 0.4\n",
    "\n",
    "# 공분산 계산\n",
    "cov_matrix = np.array([\n",
    "    [std_dev ** 2, correlation * std_dev ** 2],\n",
    "    [correlation * std_dev ** 2, std_dev ** 2]\n",
    "])\n",
    "\n",
    "# 다변수 정규분포 샘플 생성\n",
    "num_samples = 2000\n",
    "samples = np.random.multivariate_normal([mean1, mean2], cov_matrix, num_samples)\n",
    "\n",
    "# 생성된 샘플 확인\n",
    "print(\"Generated samples:\\n\", samples)\n",
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_returns[name1] = samples[:, 0]\n",
    "stock_returns[name2] = samples[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 산점도 그리기\n",
    "plt.scatter(stock_returns[name1], stock_returns[name2], alpha=0.5)\n",
    "plt.title(f'{name1}/{name2} daily returns')\n",
    "plt.xlabel(name1)\n",
    "plt.ylabel(name2)\n",
    "plt.xlim(-0.2, 0.2)\n",
    "plt.ylim(-0.3, 0.3)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "bin_edges = np.linspace(cut_left, cut_right, num_bins+1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.hist(stock_returns[name1], bins=bin_edges, alpha=0.7, label=name1)\n",
    "plt.hist(stock_returns[name2], bins=bin_edges, alpha=0.7, label=name2)\n",
    "\n",
    "plt.title(f'Distribution of Daily Returns: {name1} vs. {name2}')\n",
    "plt.xlabel('Daily Return')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean of {name1}: {stock_returns[name1].mean():.4f}\")\n",
    "print(f\"Mean of {name2}: {stock_returns[name2].mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## risk-return space에 그려보자\n",
    "앞으로 우리가 뽑을 최종 결과물이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "c = []\n",
    "for k in np.linspace(0, 1, 100):\n",
    "    merged_returns = k * stock_returns[name1] + (1 - k) * stock_returns[name2]\n",
    "    x.append(merged_returns.std())\n",
    "    y.append(merged_returns.mean())\n",
    "    c.append(k)\n",
    "\n",
    "plt.scatter(x, y, c=c, cmap='viridis')\n",
    "plt.xlabel('Standard Deviation')\n",
    "plt.ylabel('Mean')\n",
    "plt.title(f'std-mean plot {name1}/{name2}')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 생성\n",
    "\n",
    "linespace로 균일하게 내분해서 절반, uniform으로 절반 생성한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "def add_dataset(alpha):\n",
    "    # 첫번째 주식을 alpha, 두번째 주식을 1-alpha만큼.\n",
    "    bins = np.linspace(-0.1, 0.1, num_bins + 1)\n",
    "    returns = alpha * stock_returns[name1] + (1 - alpha) * stock_returns[name2]\n",
    "    hist, _ = np.histogram(returns, bins=bins)\n",
    "    dataset.append(hist/sum(hist))\n",
    "\n",
    "for t in np.linspace(0, 1, num_dataset//2): # 절반은 직접 내분\n",
    "    add_dataset(t)\n",
    "for t in np.random.uniform(0, 1, num_dataset//2): # 절반은 랜덤\n",
    "    add_dataset(t)\n",
    "\n",
    "dataset = np.array(dataset)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting torch device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_qubits = 4\n",
    "code_qubits = 1\n",
    "output_qubits = 4 # 실제로 출력 상태를 만드는데 쓰이는 개수\n",
    "n_qubits = noise_qubits\n",
    "assert(output_qubits <= n_qubits)\n",
    "\n",
    "n_layers = 10\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "print(\"n_qubits = {} n_layers = {}\".format(n_qubits, n_layers))\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits) # 제너레이터 돌려서 값 뽑아내는데 쓰임. 이중 몇개 골라서 판별자도 학습함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 0.05\n",
    "\n",
    "def generator_init(generator_input):\n",
    "    for i in range(n_qubits):\n",
    "        qml.RY((generator_input[i]-0.5) * np.pi/4, wires=i) # TODO: *a 해서 값 범위 맞추기\n",
    "\n",
    "def generator_layer(params):\n",
    "    for i in range(n_qubits):\n",
    "        qml.RY(params[i][0], wires=i)\n",
    "    for i in range(n_qubits):\n",
    "        qml.CZ(wires=[i, (i+1)%n_qubits])\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def generator_circuit(params, generator_input):\n",
    "    generator_init(generator_input)\n",
    "    for param in params:\n",
    "        generator_layer(param)\n",
    "    return qml.probs(wires=range(output_qubits))\n",
    "\n",
    "\n",
    "def generator_forward(params, generator_input):\n",
    "    generator_output = [generator_circuit(params, single_in) for single_in in generator_input] # (BATCH_SIZE, 2**output_qubits) 차원\n",
    "    generator_output = torch.stack(generator_output) # (BATCH_SIZE, 2**output_qubits) 차원\n",
    "    return generator_output\n",
    "\n",
    "\n",
    "def generator_train_step(params, generator_input, use_mine = False, _qmine = False):\n",
    "    code_input = generator_input[:, -code_qubits:]\n",
    "\n",
    "    generator_output = generator_forward(params, generator_input)\n",
    "    generator_output = generator_output.to(torch.float32) # (BATCH_SIZE, output_qubits)\n",
    "    \n",
    "    disc_output = discriminator(generator_output) # 밑에 코드에서 정의됨\n",
    "    gan_loss = -torch.log(disc_output).mean()\n",
    "    \n",
    "\n",
    "    if use_mine:\n",
    "        pred_xy = mine(code_input, generator_output)\n",
    "        code_input_shuffle = code_input[torch.randperm(BATCH_SIZE)]\n",
    "        pred_x_y = mine(code_input_shuffle, generator_output)\n",
    "        mi = torch.mean(pred_xy) - torch.log(torch.mean(torch.exp(pred_x_y)))\n",
    "        gan_loss -= coeff * mi\n",
    "\n",
    "\n",
    "    return generator_output, gan_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class LinearDiscriminator(nn.Module):\n",
    "    def __init__(self, input_dim=num_bins):\n",
    "        super().__init__()\n",
    "        self.hidden_size = 100\n",
    "        self.input_dim = input_dim\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_bins, self.hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(self.hidden_size, 1),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class LinearMine(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearMine, self).__init__()\n",
    "        H = 200\n",
    "        self.fc1 = nn.Linear(code_qubits, H)\n",
    "        self.fc2 = nn.Linear(2**output_qubits, H)\n",
    "        self.fc3 = nn.Linear(H, 1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        h1 = F.relu(self.fc1(x)+self.fc2(y))\n",
    "        h2 = self.fc3(h1)\n",
    "        return h2\n",
    "\n",
    "\n",
    "disc_loss_fn = nn.BCELoss()\n",
    "def disc_cost_fn(real_input, fake_input, smoothing=False):\n",
    "    batch_num = real_input.shape[0]\n",
    "\n",
    "    disc_real = discriminator(real_input)\n",
    "    disc_fake = discriminator(fake_input)\n",
    "\n",
    "    real_label = torch.ones((batch_num, 1)).to(device)\n",
    "    fake_label = torch.zeros((batch_num, 1)).to(device)\n",
    "    \n",
    "    if smoothing:\n",
    "        real_label = real_label - 0.2*torch.rand(real_label.shape).to(device)\n",
    "    \n",
    "    loss = 0.5 * (disc_loss_fn(disc_real, real_label) + disc_loss_fn(disc_fake, fake_label))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_params = Variable(torch.tensor(np.random.normal(-np.pi, np.pi, (n_layers, n_qubits, 1))), requires_grad=True)\n",
    "print(\"parameter shape: \", generator_params.shape)\n",
    "discriminator = LinearDiscriminator()\n",
    "mine = LinearMine()\n",
    "\n",
    "G_lr = 4e-4\n",
    "D_lr = 4e-5\n",
    "M_lr = 1e-3\n",
    "use_mine = True\n",
    "use_qmine = False\n",
    "G_opt = torch.optim.Adam([generator_params], lr=G_lr)\n",
    "D_opt = torch.optim.Adam(discriminator.parameters(), lr=D_lr)\n",
    "M_opt = torch.optim.Adam(mine.parameters(), lr=M_lr)\n",
    "\n",
    "G_scheduler = torch.optim.lr_scheduler.StepLR(G_opt, step_size=30, gamma=0.7)\n",
    "D_scheduler = torch.optim.lr_scheduler.StepLR(D_opt, step_size=30, gamma=0.85)\n",
    "M_scheduler = torch.optim.lr_scheduler.StepLR(M_opt, step_size=30, gamma=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "############# 가장 최신 버전 폴더로 저장한다 #############\n",
    "file_num = 1\n",
    "title = f'mine={use_mine}_try={file_num}'\n",
    "if not os.path.exists(f'result/{title}'):\n",
    "    os.makedirs(f'result/{title}')\n",
    "else:\n",
    "    while os.path.exists(f'result/{title}'):\n",
    "        file_num += 1\n",
    "        title = f'mine={use_mine}_try={file_num}'\n",
    "        if not os.path.exists(f'result/{title}'):\n",
    "            os.makedirs(f'result/{title}')\n",
    "            break\n",
    "###################################################\n",
    "\n",
    "with open(f'result/{title}/param.txt', 'w') as f:\n",
    "    f.write('사용주식1 = {} 사용주식2 = {}\\n'.format(name1,name2))\n",
    "    f.write('G_lr = {}\\n'.format(G_lr))\n",
    "    f.write('D_lr = {}\\n'.format(D_lr))\n",
    "    f.write('M_lr = {}\\n'.format(M_lr))\n",
    "    f.write('G_scheduler: step={}, gamma={}\\n'.format(G_scheduler.step_size, G_scheduler.gamma))\n",
    "    f.write('D_scheduler: step={}, gamma={}\\n'.format(D_scheduler.step_size, D_scheduler.gamma))\n",
    "    f.write('M_scheduler: step={}, gamma={}\\n'.format(M_scheduler.step_size, M_scheduler.gamma))\n",
    "    f.write('coeff = {}\\n'.format(coeff))\n",
    "    f.write('use_mine = {}\\n'.format(use_mine))\n",
    "    f.write('use_qmine = {}\\n'.format(use_qmine))\n",
    "    f.write('n_qubits = {}\\n'.format(n_qubits))\n",
    "    f.write('noise_qubits = {}\\n'.format(noise_qubits))\n",
    "    f.write('code_qubits = {}\\n'.format(code_qubits))\n",
    "    f.write('n_layers = {}\\n'.format(n_layers))\n",
    "    f.write('param shape = {}\\n'.format(generator_params.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_output(gen_outputs, gen_codes, means, stds, title, rep, recorder):\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "    random_indices = np.random.choice(gen_outputs.shape[0], size=9, replace=False)\n",
    "    selected_samples = [gen_outputs[i] for i in random_indices]\n",
    "    selected_codes = [gen_codes[i][0] for i in random_indices]\n",
    "\n",
    "    sorted_indices = np.argsort(selected_codes)\n",
    "    sorted_selected_codes = [selected_codes[i] for i in sorted_indices]\n",
    "    sorted_selected_samples = [selected_samples[i] for i in sorted_indices]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    for i, sample in enumerate(sorted_selected_samples):\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.bar(np.linspace(-0.1, 0.1, num_bins), sample, width=0.2*(num_bins+1)/num_bins**2)\n",
    "        plt.title(f'{sorted_selected_codes[i]:.3f}')\n",
    "        plt.ylim(0, 0.3)\n",
    "        plt.xlabel('Return')\n",
    "        plt.ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.suptitle(f\"Epoch {rep}, random imgs\")\n",
    "    plt.savefig(f'result/{title}/samples_{rep}.png')\n",
    "    if rep % 5 == 0:\n",
    "        plt.show()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.title('Epoch {0} recorder'.format(rep))\n",
    "    plt.plot(recorder['mi'], label='mi', marker='o')\n",
    "    plt.plot(recorder['d_loss'], label='d_loss', marker='o')\n",
    "    plt.plot(recorder['g_loss'], label='g_loss', marker='o')\n",
    "\n",
    "    for i in range(code_qubits):\n",
    "        plt.plot(recorder[f'c{i}-mean'], label=f'c{i}-mean', marker='o')\n",
    "        plt.plot(recorder[f'c{i}-std'], label=f'c{i}-std', marker='o')\n",
    "\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('values')\n",
    "    plt.ylim(-1, 1)\n",
    "    plt.title(f'record (rep={rep})')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'result/{title}/record_{rep}.png', dpi=100)\n",
    "    if rep % 5 == 0:\n",
    "        plt.show()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(6 + 4 * code_qubits, 4))  # 전체 그림의 크기 지정\n",
    "    for i in range(code_qubits):\n",
    "        plt.subplot(1, code_qubits, i+1)\n",
    "        plt.title('Epoch {0} code {1}'.format(rep, i))\n",
    "        plt.scatter(stds, means, s=15, c=gen_codes[:, i], cmap='RdYlBu', alpha=0.6)\n",
    "        plt.colorbar()  # 색상 막대 추가\n",
    "        plt.grid()\n",
    "    \n",
    "    plt.savefig(f'result/{title}/mean_std_{rep}.png', dpi=100)\n",
    "    if rep % 5 == 0:\n",
    "        plt.show()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "def calculate_mean_std(probs):\n",
    "    d = (cut_right - cut_left) / num_bins * (num_bins // 2 - 0.5) # 히스토그램 오른쪽 끝 중심값\n",
    "    vals = np.linspace(-d, d, num_bins)\n",
    "    mean = np.sum(probs * vals) # 평균 계산\n",
    "    std = np.sqrt(np.sum(probs * (vals - mean)**2)) # 표준편차 계산\n",
    "    return mean, std\n",
    "\n",
    "def calculate_corr(gen_outputs, code_inputs):\n",
    "    # gen_outputs: (B, bin_num), code_inputs: (B, code_qubits)\n",
    "    means = []\n",
    "    stds = []\n",
    "    for i in range(len(gen_outputs)):\n",
    "        mean, std = calculate_mean_std(gen_outputs[i])\n",
    "        means.append(mean)\n",
    "        stds.append(std)\n",
    "    mean_corrs = []\n",
    "    std_corrs = []\n",
    "    for i in range(code_qubits):\n",
    "        mean_corrs.append(np.corrcoef(means, code_inputs[:,i])[0,1])\n",
    "        std_corrs.append(np.corrcoef(stds, code_inputs[:,i])[0,1])\n",
    "    \n",
    "    return mean_corrs, std_corrs, means, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "epoch = 1500\n",
    "\n",
    "recorder_keywords = ['d_loss', 'g_loss', 'mi']\n",
    "for i in range(code_qubits):\n",
    "    recorder_keywords.append(f'c{i}-mean')\n",
    "    recorder_keywords.append(f'c{i}-std')\n",
    "recorder = {k: [] for k in recorder_keywords}\n",
    "final_rep = 0\n",
    "\n",
    "for rep in range(1, epoch+1):\n",
    "    np.random.shuffle(dataset)\n",
    "    iter_num = int(len(dataset) * 0.25 //BATCH_SIZE) # 매번 50% 추출해서 학습. 셔플하니까 자투리 생기는건 무시.\n",
    "\n",
    "    G_loss_sum = 0.0\n",
    "    D_loss_sum = 0.0\n",
    "    mi_sum = 0.0\n",
    "\n",
    "    log_gen_outputs = []\n",
    "    log_gen_codes = []\n",
    "    pbar = tqdm(range(iter_num))\n",
    "\n",
    "    for i in pbar:\n",
    "        batch = torch.FloatTensor(dataset[BATCH_SIZE * i : BATCH_SIZE * i + BATCH_SIZE])\n",
    "\n",
    "        # train generator\n",
    "        generator_seed = torch.rand((BATCH_SIZE, n_qubits))\n",
    "        generator_output, generator_loss = generator_train_step(generator_params, generator_seed, use_mine=use_mine)\n",
    "        G_opt.zero_grad()\n",
    "        generator_loss.requires_grad_(True)\n",
    "        generator_loss.backward()\n",
    "        G_opt.step()\n",
    "\n",
    "        # train discriminator\n",
    "        fake_input = generator_output.detach().to(torch.float32)\n",
    "        disc_loss = disc_cost_fn(batch, fake_input, smoothing=False)\n",
    "        D_opt.zero_grad()\n",
    "        disc_loss.requires_grad_(True)\n",
    "        disc_loss.backward()\n",
    "        D_opt.step()\n",
    "\n",
    "        # train mine\n",
    "        code_input = generator_seed[:, -code_qubits:] \n",
    "        pred_xy = mine(code_input, fake_input)\n",
    "        code_input_shuffle = code_input[torch.randperm(BATCH_SIZE)]\n",
    "        pred_x_y = mine(code_input_shuffle, fake_input)\n",
    "        mi = -torch.mean(pred_xy) + torch.log(torch.mean(torch.exp(pred_x_y)))\n",
    "        M_opt.zero_grad()\n",
    "        mi.requires_grad_(True)\n",
    "        mi.backward()\n",
    "        M_opt.step()\n",
    "\n",
    "        D_loss_sum += disc_loss.item()\n",
    "        G_loss_sum += generator_loss.item()\n",
    "        mi_sum -= mi.item() # (-1)곱해져 있어서 빼야함.\n",
    "\n",
    "        pbar.set_postfix({'G_loss': G_loss_sum/(i+1), 'D_loss': D_loss_sum/(i+1), 'MI': mi_sum/(i+1)})\n",
    "        log_gen_outputs.append(fake_input.numpy())\n",
    "        log_gen_codes.append(code_input.numpy())\n",
    "\n",
    "    G_scheduler.step()\n",
    "    D_scheduler.step()\n",
    "    M_scheduler.step()\n",
    "\n",
    "    recorder['d_loss'].append(D_loss_sum/iter_num)\n",
    "    recorder['g_loss'].append(G_loss_sum/iter_num)\n",
    "    recorder['mi'].append(mi_sum/iter_num)\n",
    "\n",
    "    print(\"epoch: {}, D_loss: {}, G_loss: {}, MI = {}\".format(rep, D_loss_sum/iter_num, G_loss_sum/iter_num, mi_sum/iter_num))\n",
    "    \n",
    "    log_gen_outputs = np.concatenate(log_gen_outputs, axis=0)\n",
    "    log_gen_codes = np.concatenate(log_gen_codes, axis=0)\n",
    "    \n",
    "    mean_corrs, std_corrs, means, stds = calculate_corr(log_gen_outputs, log_gen_codes)\n",
    "    for i in range(code_qubits):\n",
    "        recorder[f'c{i}-mean'].append(mean_corrs[i])\n",
    "        recorder[f'c{i}-std'].append(std_corrs[i])\n",
    "\n",
    "    visualize_output(log_gen_outputs, log_gen_codes, means, stds, title, rep, recorder)\n",
    "    \n",
    "    with open(f'result/{title}/Opts_{rep}.pkl', 'wb') as file:\n",
    "        pickle.dump((G_opt, D_opt, M_opt), file)\n",
    "    with open(f'result/{title}/schedulers_{rep}.pkl', 'wb') as file:\n",
    "        pickle.dump((G_scheduler, D_scheduler, M_scheduler), file)\n",
    "    with open(f'result/{title}/discriminator_{rep}.pkl', 'wb') as file:\n",
    "        pickle.dump(discriminator, file)\n",
    "    with open(f'result/{title}/generator_{rep}.pkl', 'wb') as file:\n",
    "        pickle.dump(generator_params, file)\n",
    "\n",
    "    df = pd.DataFrame(recorder)\n",
    "    output_filename = f'result/{title}/recorder.xlsx'\n",
    "    df.to_excel(output_filename, index=False)\n",
    "\n",
    "    final_rep = rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최종 결과 plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "for i in tqdm(range(5000)):\n",
    "    with torch.no_grad():\n",
    "        z = np.random.uniform(-1, 1, (1, n_qubits, 1))\n",
    "        code_input = z[:, -code_qubits:].reshape(code_qubits) # 입력 z중에서 code를 추출한다.\n",
    "        generator_output = generator_forward(generator_params, z, copula=output_copula)\n",
    "        generator_output = generator_output.cpu().numpy().reshape(2)\n",
    "        outputs.append(generator_output)\n",
    "        inputs.append(code_input)\n",
    "\n",
    "inputs = np.array(inputs).reshape(-1, code_qubits)\n",
    "\n",
    "for code_ind in range(code_qubits):\n",
    "    outputs = np.array(outputs)\n",
    "    plt.scatter(outputs[:, 0], outputs[:, 1], c=inputs[:, code_ind], cmap='RdYlBu', alpha=0.2)\n",
    "    plt.colorbar()  # 색상 막대 추가\n",
    "    plt.title(f'code{code_ind}-distribution (rep = {final_rep})')\n",
    "    plt.savefig(f'result/{title}/code_{code_ind}_{final_rep}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pennylane",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
