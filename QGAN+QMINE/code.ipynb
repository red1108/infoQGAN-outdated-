{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from functools import reduce\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pennylane import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_points_distribution(data_num=2000, dist_type=\"circle\"):\n",
    "    if dist_type == 'spiral':\n",
    "        theta = np.linspace(0, 5*np.pi, data_num)  # 각도 범위를 정의합니다.\n",
    "        radius = np.linspace(0, 0.3, data_num)  # 반지름 범위를 정의합니다.\n",
    "        xx = 0.5 + radius * np.cos(theta) + 0.012 * np.random.randn(data_num)  # x 좌표 계산\n",
    "        yy = 0.5 + radius * np.sin(theta) + 0.012 * np.random.randn(data_num)  # y 좌표 계산\n",
    "    \n",
    "    elif dist_type == 'box':\n",
    "        xx = np.random.uniform(0.3, 0.7, data_num) + 0.012 * np.random.randn(data_num)  # x 좌표 계산\n",
    "        yy = np.random.uniform(0.3, 0.7, data_num) + 0.012 * np.random.randn(data_num)  # y 좌표 계산\n",
    "\n",
    "    elif dist_type == 'curve':\n",
    "        xx = np.linspace(0.15, 0.85, data_num) + 0.02 * np.random.randn(data_num)  # x 좌표 계산\n",
    "        yy = 3.8 * xx*xx - 3.8 * xx + 1.2 + 0.03 * np.random.randn(data_num)  # y 좌표 계산\n",
    "    \n",
    "    elif dist_type == 'circle':\n",
    "        radius = 0.20\n",
    "        theta = np.linspace(0, 2 * np.pi, data_num)  # 각도 범위를 정의합니다.\n",
    "        xx = radius * np.cos(theta) + 0.5 + 0.02 * np.random.randn(data_num)  # x 좌표 계산\n",
    "        yy = radius * np.sin(theta) + 0.55 + 0.02 * np.random.randn(data_num)  # y 좌표 계산\n",
    "\n",
    "    elif dist_type == 'lemniscate':\n",
    "        xx = []\n",
    "        yy = []\n",
    "        while len(xx) < data_num:\n",
    "            x = np.random.uniform(0, 0.5)\n",
    "            y = np.random.uniform(0, 0.5)\n",
    "            \n",
    "            r = (x**2 + y**2)**0.5\n",
    "            theta = np.arctan2(y, x)\n",
    "            if r**2 <= 2 * 0.25**2 * np.cos(2 * theta):\n",
    "                x = x if np.random.uniform(0, 1) < 0.5 else -x\n",
    "                y = y if np.random.uniform(0, 1) < 0.5 else -y\n",
    "                xx.append(x+0.5)\n",
    "                yy.append(y+0.5)\n",
    "\n",
    "    data = np.column_stack((xx, yy))  # x와 y 좌표를 합쳐서 데이터 생성\n",
    "    np.random.shuffle(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = generate_points_distribution(2000, 'box')\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(x[:,0], x[:,1], s=2.0)\n",
    "plt.title('Target distribution')\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((0, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting torch device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_qubits = 4\n",
    "code_qubits = 1\n",
    "n_qubits = noise_qubits + code_qubits\n",
    "output_qubits = 2\n",
    "assert(output_qubits <= noise_qubits) # 출력 큐빗은 noise qubit이하여야 한다.\n",
    "\n",
    "n_layers = 20\n",
    "BATCH_SIZE = 16\n",
    "print(\"n_qubits = {} n_layers = {}\".format(n_qubits, n_layers))\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "# dev = qml.device(\"ionq.simulator\", wires=2)\n",
    "# dev = qml.device(\"ionq.qpu\", wires=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 0.2\n",
    "\n",
    "def generator_init(generator_input):\n",
    "    for i in range(n_qubits):\n",
    "        qml.RY(generator_input[i]*np.pi/2, wires=i) # TODO: *a 해서 값 범위 맞추기\n",
    "\n",
    "def generator_layer(params):\n",
    "    for i in range(n_qubits):\n",
    "        qml.RX(params[i][0], wires=i)\n",
    "        qml.RY(params[i][0], wires=i)\n",
    "        qml.RZ(params[i][1], wires=i)\n",
    "    \n",
    "    for i in range(n_qubits):\n",
    "        qml.CNOT(wires=[i, (i+1)%n_qubits])\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def generator_circuit(params, generator_input):\n",
    "    \"\"\"\n",
    "    quantum circuit nodeq1\n",
    "    generator_input (np.array(큐빗)) : 생성기 입력 seed (noise + code)\n",
    "    params (torch.Tensor(레이어,큐빗,3)): a parameter\n",
    "    마지막 측정은 모두 Z로\n",
    "    \"\"\"\n",
    "\n",
    "    generator_init(generator_input)\n",
    "\n",
    "    for param in params:\n",
    "        generator_layer(param)\n",
    "\n",
    "    return [qml.probs(wires=i) for i in range(n_qubits)]\n",
    "\n",
    "def generator_forward(params, generator_input):\n",
    "    # 제너레이터 돌리고 결과 return하는 함수\n",
    "    generator_output = [generator_circuit(params, single_in)[::2] for single_in in generator_input]\n",
    "    generator_output = torch.stack(generator_output) # (BATCH_SIZE, n_qubits) 차원\n",
    "    generator_output = 2 * (2 / np.pi * torch.arcsin(torch.sqrt(generator_output))) -0.5 # (BATCH_SIZE, n_qubits) 차원\n",
    "    # 곱하기 2 해서 범위를 초과하게 만들음\n",
    "\n",
    "    return generator_output[:, :output_qubits], generator_output[:, -code_qubits:] # noise, code 순서로 반환\n",
    "\n",
    "\n",
    "def generator_train_step(params, generator_input, use_mine = False, _qmine = False):\n",
    "    '''\n",
    "    params (torch.Tensor(레이어,큐빗,3)): a parameter\n",
    "    generator_input (torch.Tensor(BATCH_SIZE, n_qubits)): 생성기 입력 seed (noise + code). -1~1 사이의 값\n",
    "    '''\n",
    "    code_input = generator_input[:, -code_qubits:] # 입력중에서 code만 뽑는다. (BATCH_SIZE, code_qubits)\n",
    "\n",
    "    generator_output, code_output = generator_forward(params, generator_input) # 출력을 뽑아낸다\n",
    "    generator_output = generator_output.to(torch.float32) # (BATCH_SIZE, output_qubits)\n",
    "    \n",
    "    disc_output = discriminator(generator_output) # 밑에 코드에서 정의됨\n",
    "    gan_loss = torch.log(1-disc_output).mean()\n",
    "    # print(\"gan_loss = \", gan_loss, gan_loss.shape)\n",
    "    \n",
    "    t = (code_input - code_output).pow(2).mean()\n",
    "\n",
    "    if use_mine:\n",
    "        pred_xy = mine(code_input, generator_output)\n",
    "        code_input_shuffle = code_input[torch.randperm(BATCH_SIZE)]\n",
    "        pred_x_y = mine(code_input_shuffle, generator_output)\n",
    "        mi = torch.mean(pred_xy) - torch.log(torch.mean(torch.exp(pred_x_y)))\n",
    "        gan_loss -= coeff * mi\n",
    "\n",
    "    elif _qmine:\n",
    "        gan_loss += 0 # TODO: qmine loss 추가하기\n",
    "\n",
    "    return generator_output, gan_loss# TODO: 이건 분석용으로 넣어놓음.지워야 함.\n",
    "\n",
    "\n",
    "def prediction(params, image, debug=False):\n",
    "    prob_0 = 0\n",
    "    prob_1 = 0\n",
    "    for i in range(n_layers):\n",
    "        prob_0 += circuit(params[:i+1], (image, 0))\n",
    "        prob_1 += circuit(params[:i+1], (image, 1))\n",
    "\n",
    "    if (debug):\n",
    "        return (int(prob_0 <= prob_1), prob_0, prob_1)\n",
    "    return int(prob_0 <= prob_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class LinearDiscriminator(nn.Module):\n",
    "    def __init__(self, input_dim=output_qubits):\n",
    "        super().__init__()\n",
    "        self.hidden_size = 20\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, self.hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(self.hidden_size, 1),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        if(len(x.shape) != 2):\n",
    "            x = x.view(x.shape[0], -1)\n",
    "\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class LinearMine(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearMine, self).__init__()\n",
    "        H = 10\n",
    "        self.fc1 = nn.Linear(code_qubits, H)\n",
    "        self.fc2 = nn.Linear(output_qubits, H)\n",
    "        self.fc3 = nn.Linear(H, 1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        h1 = F.relu(self.fc1(x)+self.fc2(y))\n",
    "        h2 = self.fc3(h1)\n",
    "        return h2\n",
    "\n",
    "disc_loss_fn = nn.BCELoss()\n",
    "def disc_cost_fn(real_input, fake_input, smoothing=False):\n",
    "    batch_num = real_input.shape[0]\n",
    "\n",
    "    disc_real = discriminator(real_input)\n",
    "    disc_fake = discriminator(fake_input)\n",
    "\n",
    "    real_label = torch.ones((batch_num, 1)).to(device)\n",
    "    fake_label = torch.zeros((batch_num, 1)).to(device)\n",
    "    \n",
    "    if smoothing:\n",
    "        real_label = real_label - 0.2*torch.rand(real_label.shape).to(device)\n",
    "    \n",
    "    loss = 0.5 * (disc_loss_fn(disc_real, real_label) + disc_loss_fn(disc_fake, fake_label))\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_params = Variable(torch.tensor(np.random.normal(-np.pi, np.pi, (n_layers, n_qubits, 2))), requires_grad=True)\n",
    "print(\"parameter shape: \", generator_params.shape)\n",
    "\n",
    "discriminator = LinearDiscriminator()\n",
    "mine = LinearMine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_lr = 1e-3\n",
    "D_lr = 3e-4\n",
    "M_lr = 1e-3\n",
    "use_mine = True\n",
    "use_qmine = False\n",
    "G_opt = torch.optim.Adam([generator_params], lr=G_lr)\n",
    "D_opt = torch.optim.Adam(discriminator.parameters(), lr=D_lr)\n",
    "M_opt = torch.optim.Adam(mine.parameters(), lr=M_lr)\n",
    "\n",
    "G_scheduler = torch.optim.lr_scheduler.StepLR(G_opt, step_size=30, gamma=0.7)\n",
    "D_scheduler = torch.optim.lr_scheduler.StepLR(D_opt, step_size=30, gamma=0.85)\n",
    "M_scheduler = torch.optim.lr_scheduler.StepLR(M_opt, step_size=30, gamma=0.7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하이퍼파라미터 미리 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "title = f'square1_{use_mine}'\n",
    "if not os.path.exists(f'result/{title}'):\n",
    "    os.makedirs(f'result/{title}')\n",
    "    \n",
    "with open(f'result/{title}/param.txt', 'w') as f:\n",
    "    f.write('G_lr = {}\\n'.format(G_lr))\n",
    "    f.write('D_lr = {}\\n'.format(D_lr))\n",
    "    f.write('M_lr = {}\\n'.format(M_lr))\n",
    "    f.write('G_scheduler: step={}, gamma={}\\n'.format(G_scheduler.step_size, G_scheduler.gamma))\n",
    "    f.write('D_scheduler: step={}, gamma={}\\n'.format(D_scheduler.step_size, D_scheduler.gamma))\n",
    "    f.write('M_scheduler: step={}, gamma={}\\n'.format(M_scheduler.step_size, M_scheduler.gamma))\n",
    "    f.write('coeff = {}\\n'.format(coeff))\n",
    "    f.write('use_mine = {}\\n'.format(use_mine))\n",
    "    f.write('use_qmine = {}\\n'.format(use_qmine))\n",
    "    f.write('n_qubits = {}\\n'.format(n_qubits))\n",
    "    f.write('output_qubits = {}\\n'.format(output_qubits))\n",
    "    f.write('code_qubits = {}\\n'.format(code_qubits))\n",
    "    f.write('n_layers = {}\\n'.format(n_layers))\n",
    "    f.write('param shape = {}\\n'.format(generator_params.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "def visualize_output(log_gen_outputs, log_gen_codes, title, rep, recorder):\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    plt.figure(figsize=(10 + 4 * code_qubits, 4))  # 전체 그림의 크기 지정\n",
    "    plt.subplot(1, 1 + code_qubits, 1)\n",
    "    plt.title('Epoch {0}'.format(rep))\n",
    "    plt.scatter(log_gen_outputs[:,0], log_gen_outputs[:,1], s=10,  alpha=0.2)\n",
    "    plt.xlim((0, 1))\n",
    "    plt.ylim((0, 1))\n",
    "    plt.grid()\n",
    "\n",
    "    for i in range(code_qubits):\n",
    "        plt.subplot(1, 1 + code_qubits, 2 + i)\n",
    "        plt.title('Epoch {0} code {1}'.format(rep, i))\n",
    "        plt.scatter(log_gen_outputs[:,0], log_gen_outputs[:,1], s=10, c=log_gen_codes[:, i], cmap='RdYlBu', alpha=0.2)\n",
    "        plt.xlim((0, 1))\n",
    "        plt.ylim((0, 1))\n",
    "        plt.colorbar()  # 색상 막대 추가\n",
    "        plt.grid()\n",
    "\n",
    "    plt.savefig(f'result/{title}/{rep}.png', dpi=100)\n",
    "    if rep % 5 == 0:\n",
    "        plt.show()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.title('Epoch {0} code-axis corr'.format(rep))\n",
    "    for i in range(code_qubits):\n",
    "        plt.plot(recorder[f'code{i}-x'], label=f'code{i}-x', marker='o')\n",
    "        plt.plot(recorder[f'code{i}-y'], label=f'code{i}-y', marker='o')\n",
    "\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('correlation')\n",
    "    plt.ylim(-1, 1)\n",
    "    plt.title(f'code - axis corr graph (rep={rep})')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'result/{title}/corr_{rep}.png', dpi=100)\n",
    "    if rep % 5 == 0:\n",
    "        plt.show()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.title('Epoch {0} KS stastics'.format(rep))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(recorder['D_ks'], label='D_ks', marker='o')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(recorder['p-value'], label='p-value', marker='o')\n",
    "    plt.yscale(\"log\", base=10)  # 로그 스케일로 설정\n",
    "    plt.legend()\n",
    "    plt.savefig(f'result/{title}/ks_{rep}.png', dpi=100)\n",
    "    if rep % 5 == 0:\n",
    "        plt.show()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ndtest\n",
    "import openpyxl\n",
    "epoch = 300\n",
    "\n",
    "recorder_keywords = ['d_loss', 'g_loss', 'mi', 'D_ks', 'p-value']\n",
    "for i in range(code_qubits):\n",
    "    recorder_keywords.append(f'code{i}-x')\n",
    "    recorder_keywords.append(f'code{i}-y')\n",
    "\n",
    "recorder = {k: [] for k in recorder_keywords}\n",
    "final_rep = 0\n",
    "\n",
    "for rep in range(1, epoch+1):\n",
    "    np.random.shuffle(x)\n",
    "    iter_num = int(len(x)*0.25//BATCH_SIZE) # 매번 50% 추출해서 학습. 셔플하니까 자투리 생기는건 무시.\n",
    "    \n",
    "    G_loss_sum = 0.0\n",
    "    D_loss_sum = 0.0\n",
    "    mi_sum = 0.0\n",
    "    pbar = tqdm(range(iter_num))\n",
    "    log_gen_outputs = []\n",
    "    log_gen_codes = []\n",
    "    \n",
    "    for i in pbar:\n",
    "        batch = torch.FloatTensor(x[BATCH_SIZE * i : BATCH_SIZE * i + BATCH_SIZE])\n",
    "        \n",
    "        # train generator\n",
    "        generator_seed = torch.rand((BATCH_SIZE, n_qubits)) * 2 - 1\n",
    "        generator_output, generator_loss = generator_train_step(generator_params, generator_seed, use_mine=use_mine, _qmine=use_qmine)\n",
    "        G_opt.zero_grad()\n",
    "        generator_loss.requires_grad_(True)\n",
    "        generator_loss.backward()\n",
    "        G_opt.step()\n",
    "\n",
    "        # train discriminator\n",
    "        fake_input = generator_output.detach().to(torch.float32)\n",
    "        disc_loss = disc_cost_fn(batch, fake_input, smoothing=False)\n",
    "        D_opt.zero_grad()\n",
    "        disc_loss.requires_grad_(True)\n",
    "        disc_loss.backward()\n",
    "        D_opt.step()\n",
    "\n",
    "        # train mine\n",
    "        code_input = generator_seed[:, -code_qubits:] \n",
    "        pred_xy = mine(code_input, fake_input)\n",
    "        code_input_shuffle = code_input[torch.randperm(BATCH_SIZE)]\n",
    "        pred_x_y = mine(code_input_shuffle, fake_input)\n",
    "        mi = -torch.mean(pred_xy) + torch.log(torch.mean(torch.exp(pred_x_y)))\n",
    "        M_opt.zero_grad()\n",
    "        mi.requires_grad_(True)\n",
    "        mi.backward()\n",
    "        M_opt.step()\n",
    "\n",
    "        D_loss_sum += disc_loss.item()\n",
    "        G_loss_sum += generator_loss.item()\n",
    "        mi_sum -= mi.item() # (-1)곱해져 있어서 빼야함.\n",
    "\n",
    "        pbar.set_postfix({'G_loss': G_loss_sum/(i+1), 'D_loss': D_loss_sum/(i+1), 'MI': mi_sum/(i+1)})\n",
    "        log_gen_outputs.append(fake_input.numpy())\n",
    "        log_gen_codes.append(code_input.numpy())\n",
    "\n",
    "    G_scheduler.step()\n",
    "    D_scheduler.step()\n",
    "    M_scheduler.step()\n",
    "\n",
    "    recorder['d_loss'].append(D_loss_sum/iter_num)\n",
    "    recorder['g_loss'].append(G_loss_sum/iter_num)\n",
    "    recorder['mi'].append(mi_sum/iter_num)\n",
    "    \n",
    "    log_gen_outputs = np.concatenate(log_gen_outputs, axis=0)\n",
    "    log_gen_codes = np.concatenate(log_gen_codes, axis=0)\n",
    "    print(\"epoch: {}, D_loss: {}, G_loss: {}, MI = {}\".format(rep, D_loss_sum/iter_num, G_loss_sum/iter_num, mi_sum/iter_num))\n",
    "    print(\"좌표값 평균 = \", np.mean(log_gen_outputs[:,0]), np.mean(log_gen_outputs[:,1]))\n",
    "\n",
    "    df = pd.DataFrame({'x': log_gen_outputs[:, 0], 'y': log_gen_outputs[:, 1]})\n",
    "    for i in range(code_qubits):\n",
    "        df[f'code{i}']=log_gen_codes[:, i]\n",
    "    corr_mat = df.corr().to_numpy()\n",
    "    for i in range(code_qubits):\n",
    "        recorder[f'code{i}-x'].append(corr_mat[0, i+2])\n",
    "        recorder[f'code{i}-y'].append(corr_mat[1, i+2])\n",
    "    \n",
    "    p_value, D_ks = ndtest.ks2d2s(log_gen_outputs[:, 0], log_gen_outputs[:, 1], x[:, 0], x[:, 1], extra=True)\n",
    "    recorder['D_ks'].append(D_ks)\n",
    "    recorder['p-value'].append(p_value)\n",
    "    print(\"D_ks = {}, p-value = {}\".format(D_ks, p_value))\n",
    "\n",
    "    visualize_output(log_gen_outputs, log_gen_codes, title, rep, recorder)\n",
    "\n",
    "    with open(f'result/{title}/discriminator_{rep}.pkl', 'wb') as file:\n",
    "        pickle.dump(discriminator, file)\n",
    "    with open(f'result/{title}/generator_{rep}.pkl', 'wb') as file:\n",
    "        pickle.dump(generator_params, file)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(recorder)\n",
    "    output_filename = f'result/{title}/recorder.xlsx'\n",
    "    df.to_excel(output_filename, index=False)\n",
    "\n",
    "    final_rep = rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target distribution 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(x[:,0], x[:,1], s=2.0)\n",
    "plt.title('Target distribution')\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((0, 1))\n",
    "plt.savefig(f'result/{title}/target.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target distribution 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(x[:,0], x[:,1], s=2.0)\n",
    "plt.title('Target distribution')\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((0, 1))\n",
    "plt.savefig(f'result/{title}/target.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 불러오는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'result/{title}/discriminator2.pkl', 'rb') as file:\n",
    "    discriminator = pickle.load(file)\n",
    "\n",
    "with open(f'result/{title}/generator.pkl', 'rb') as file:\n",
    "    generator_params = pickle.load(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
